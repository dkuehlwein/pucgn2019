{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The *Hello World* for Deep Learning\n",
    "Deep learning algorithms fundamentally changed how we approach vision and natural language processing. In this notebook we show the *hello world* for deep learning: Classification of images of single digits on the MNIST dataset.\n",
    "\n",
    "As usual, we start by importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras import utils\n",
    "from keras.datasets import mnist # the \"hello world\" data set for deep learning\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# To show plots directly in the notebook\n",
    "%matplotlib inline  \n",
    "np.random.seed(3) # set seed for reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "The MNIST dataset comes split in a train and test data set. It contains 70.000 28x28 pixel images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print('\\nExample:')\n",
    "print(y_train[17])\n",
    "plt.imshow(X_train[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Our model requires 4-dimensional inputs (nr_of_samples, height, width, channels). In the MNIST dataset, the channel is omitted since there is only one. We can make it explicit through reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y arrays contains the number that can be seen in a image. We want our model to predict a probability for each possible digit. For this, we need to convert y from 1 to 10 dimensions, where each column corresponds to one digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = utils.to_categorical(y_train, 10)\n",
    "Y_test = utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('Digit: %s' % y_train[i])\n",
    "    print('Encoding: %s' % Y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the model\n",
    "After we prepared our data for the training, we are now ready to set up our model. \n",
    "We'll use a combination of two convolutions - max pooling layers, a dense layer with dropout and finally an output layer with one neuron for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Convolution2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary gives a good overview of the overall shape of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow models need to be compiled. For this, we also need to define the loss function that will be optimized, the optimizer we want to use and any additional metrics we want to keep track of during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "Training state-of-the-art deep learning models is impossible without GPUs or other specializes hardware. \n",
    "Even our toy example requires quite some time. Hence, we lower the number of training examples. This will reduce the performance of our model, but give us quicker results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_samples = 4000\n",
    "X_train_small = X_train[1:number_of_training_samples]\n",
    "Y_train_small = Y_train[1:number_of_training_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start training our model. You can see how the loss decreases and the accuracy increases in each epoch.\n",
    "The batch-size defines how many images are used for one upgrade step. An epoch is one full run through our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_small, Y_train_small, \n",
    "          batch_size=32, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *.fit* function shows the performance on the training set. It is essential to also measure how the model performs on new, unseen data. For this, we use the *_test* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(f\"categorical_crossentropy: {score[0]}, accuracy: {score[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, the accuracy should be approximately the same on the train and test set. Let's take a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 512\n",
    "plt.imshow(np.vstack(X_test[index:index+5]).reshape([-1,28]))\n",
    "np.argmax(model.predict(X_test[index:index+5]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model only got an accuracy of around *96%*. Let's take a look at some images where it predicted the wrong digit.\n",
    "\n",
    "First, we create a mask for images where the predicition was wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_probabilities = model.predict(X_test)\n",
    "predictions = np.argmax(predictions_probabilities, axis=1)\n",
    "false_predictions_mask = predictions != y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we extract the images, probabilites, predictions and labels of the wrongly classified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pics = X_test[false_predictions_mask]\n",
    "false_predictions_probabilities = predictions_probabilities[false_predictions_mask]\n",
    "false_predictions = predictions[false_predictions_mask]\n",
    "false_labels = y_test[false_predictions_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can look at some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 5\n",
    "plt.imshow(np.vstack(false_pics[:samples]).reshape([-1,28]))\n",
    "print('Predictions')\n",
    "print(false_predictions[:samples])\n",
    "print('Labels')\n",
    "print(false_labels[:samples])\n",
    "print('Probabilities:')\n",
    "print(np.round(false_predictions_probabilities[:samples], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises\n",
    "Preprocessing can have a huge impact on the performance of the model. Normalizing the input to e.g. 0-1 usually helps with the convergance of the model. Since the default type of MNIST is uint8, we first need to convert it to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtype)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values of X are below 0 and 255 normalization can be done via a simply division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.max())\n",
    "print(X_train.min())\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# normalize to -1 to 1\n",
    "#X_train /= 127.5 - 1\n",
    "#X_test /= 127.5 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "- Test the effect of 0 - 1 and -1 - 1 normalization on the training and accuracy.\n",
    "- Achieve an accuracy > *98%* on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
