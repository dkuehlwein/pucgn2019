{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-rl\n",
    "!pip install gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Declare classes: AI Classes: Random AI, Minimax AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomAI:\n",
    "\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "\n",
    "    def decide_turn(self):\n",
    "        board = self.game.board\n",
    "        assert sum([1 for cell in board.flatten() if cell == 0]) > 0, \"no place to make a turn!!!\"\n",
    "\n",
    "        coords = [0,1,2]\n",
    "        row = random.choice(coords)\n",
    "        column = random.choice(coords)\n",
    "\n",
    "        while board[row, column] != 0:\n",
    "            row = random.choice(coords)\n",
    "            column = random.choice(coords)\n",
    "\n",
    "        return row, column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf as infinity\n",
    "\n",
    "class MinimaxAI:\n",
    "\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "        self.random_ai = RandomAI(game)\n",
    "\n",
    "    def decide_turn(self):\n",
    "        turns_possible = self.empty_cells()\n",
    "        if len(turns_possible) == 1:\n",
    "            row, column = turns_possible[0]\n",
    "        elif len(turns_possible) >= 7:\n",
    "            row, column = self.random_ai.decide_turn()\n",
    "        else:\n",
    "            chosen_node = self.minimax(10, self.game.x_next)\n",
    "            row, column, _ = chosen_node\n",
    "        return row, column\n",
    "\n",
    "    def empty_cells(self):\n",
    "        state = self.game.board\n",
    "        results = []\n",
    "        for row in range(3):\n",
    "            for column in range(3):\n",
    "                if state[row, column] == 0:\n",
    "                    results.append((row, column))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def minimax(self,depth, X_turn):\n",
    "        state = self.game.board\n",
    "        score = self.game.evaluate(state)\n",
    "        game_over = score is not None\n",
    "\n",
    "        if depth == 0 or game_over:\n",
    "            return [-1, -1, score]\n",
    "\n",
    "        if X_turn:\n",
    "            best = [-1, -1, -infinity]\n",
    "        else:\n",
    "            best = [-1, -1, +infinity]\n",
    "\n",
    "        for cell in self.empty_cells():\n",
    "            x, y = cell[0], cell[1]\n",
    "            self.game.board[x, y] = 1 if X_turn else -1\n",
    "            score = self.minimax(depth - 1, not X_turn)\n",
    "            self.game.board[x, y] = 0\n",
    "            score[0], score[1] = x, y\n",
    "\n",
    "            if X_turn:\n",
    "                if score[2] > best[2]:\n",
    "                    best = score  # max value\n",
    "            else:\n",
    "                if score[2] < best[2]:\n",
    "                    best = score  # min value\n",
    "\n",
    "        return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Environment class - the actual game implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE TTT GAME CLASS\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from functools import lru_cache\n",
    "from gym.core import Env\n",
    "from gym import spaces\n",
    "\n",
    "class TicTacToe(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros([3,3], dtype=np.int8)\n",
    "        self.x_next = True if random.random() > 0.5 else False\n",
    "\n",
    "        # by design, our AI plays x and the hard-coded AI plays for o\n",
    "        self.x_ai = None\n",
    "        self.o_ai = MinimaxAI(self)\n",
    "\n",
    "        # action space consists of 9 distinct actions - trying to place your shape to each of 9 cells.\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "\n",
    "        # observation space is the state of the board, as numpy array with possible values of\n",
    "        # -1 - cell with 'o',\n",
    "        # 0  - empty cell,\n",
    "        # 1  - cell with 'x'\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, dtype=np.int8, shape=(3,3))\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the board for the next game. Starting player is random.\n",
    "        \"\"\"\n",
    "        self.board = np.zeros([3, 3])\n",
    "        self.x_next = True if random.random() > 0.5 else False\n",
    "        if not self.x_next:\n",
    "            row, column = self.o_ai.decide_turn()\n",
    "            assert self.try_make_turn(row, column)\n",
    "\n",
    "        return self.board\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        TODO this method is not complete. its up to you to finish it.\n",
    "        Building blocks to be used:\n",
    "            1. valid_turn = self.try_make_turn(row, column) \n",
    "                valid turn means we tried to place our shape in an empty cell.\n",
    "                \n",
    "            2. result = self.evaluate(self.board)\n",
    "                result is either None if game is not over, or one of -1, 0, 1\n",
    "                \n",
    "            3. row, column = self.o_ai.decide_turn()\n",
    "                it is only the decision of the opponent ai, the turn is not made yet\n",
    "            \n",
    "            \n",
    "        :param action: a discrete action from 0 to 8. Cells are enumerated from left to right,\n",
    "        with columns continuing from top to bottom, like below:\n",
    "\n",
    "         0 | 1 | 2\n",
    "        -  + - + -\n",
    "         3 | 4 | 5\n",
    "        -  + - + -\n",
    "         6 | 7 | 8\n",
    "\n",
    "        if invalid action is used, board does not change, but negative reward is returned.\n",
    "        :return: observation, reward, done?, info (empty)\n",
    "        \"\"\"\n",
    "        assert self.x_next # verify it is your turn\n",
    "\n",
    "        # extract row and column from your actions\n",
    "\n",
    "        row = action // 3\n",
    "        column = action % 3\n",
    "\n",
    "        # Info is an empty dictionary. a dictionary is required by the keras-rl specs.\n",
    "        info = {}\n",
    "\n",
    "        # use the row and column to try to make a step\n",
    "\n",
    "        # if the turn is not a valid one, the game ends and we lose it.\n",
    "        # The reward should be negative and greater than a game lost normally.\n",
    "\n",
    "        # check it your turn has ended the game.\n",
    "        # if not, opponent should make his turn, and we check again if the game is over.\n",
    "\n",
    "        #if the game is over, we return the observation, reward = self.evaluate(self.board), done = True and the info.\n",
    "\n",
    "        # if the game is not over, we return the observation, reward = 0, done = False and the info.\n",
    "\n",
    "        # TODO default return - remove it when you implement a real one.\n",
    "        return np.zeros([3,3]), 0, False, info\n",
    "\n",
    "\n",
    "    def try_make_turn(self, row, column):\n",
    "        \"\"\"\n",
    "        The current player according to the boolean self.x_next tries to make a turn\n",
    "        by placing their shape on the crossection of the crossection of :param row and :param column\n",
    "        :return: True if it was a valid turn, False if such turn is not allowed (the cell is not empty).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.board[row,column] == 0:\n",
    "\n",
    "            self.board[row, column] = 1 if self.x_next else -1\n",
    "            self.x_next = not self.x_next\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(board):\n",
    "        \"\"\"\n",
    "        takes a :param board as the input ([3,3] numpy array),\n",
    "        :return:\n",
    "            None if the game is not over,\n",
    "            1 if x won,\n",
    "            -1 if o won,\n",
    "            0 if its a draw\n",
    "        \"\"\"\n",
    "        board_as_tuple = tuple(tuple(board[row]) for row in range(3))\n",
    "        return TicTacToe._evaluate(board_as_tuple)\n",
    "\n",
    "    def play_ai_game(self):\n",
    "        while self.evaluate(self.board) is None:\n",
    "            ai = self.x_ai if self.x_next else self.o_ai\n",
    "            row, column = ai.decide_turn()\n",
    "            valid_turn = self.try_make_turn(row, column)\n",
    "            if not valid_turn:\n",
    "                # any AI that makes invalid turn immediately loses the game.\n",
    "                return -1 if self.x_next else 1\n",
    "        return self.evaluate(self.board)\n",
    "\n",
    "    @property\n",
    "    def board_as_tuple(self):\n",
    "        return tuple(tuple(self.board[row]) for row in range(3))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=2**16)\n",
    "    def _evaluate(board):\n",
    "\n",
    "        sums = []\n",
    "        # we collect totals of all row, columns and diagonals. Any of those must have a value of\n",
    "        # either 3 or -3 if there are x x x or o o o in this sequence.\n",
    "        sums += [sum(board[row]) for row in range(3)]\n",
    "        sums += [sum([board[i][column] for i in range(3)]) for column in range(3)]\n",
    "        sum_main_diag = sum([board[i][i] for i in range(3)])\n",
    "        sum_opp_diag = sum([board[i][2 - i] for i in range(3)])\n",
    "\n",
    "        sums.append(sum_main_diag)\n",
    "        sums.append(sum_opp_diag)\n",
    "        \n",
    "        n_steps = sum(abs(elem) for row in board for elem in row)\n",
    "\n",
    "        if 3 in sums:\n",
    "            return 1 - n_steps / 100\n",
    "        elif -3 in sums:\n",
    "            return -1 + n_steps / 100\n",
    "        else:\n",
    "            n_empty = sum([1 for row in range(3) for cell in board[row] if cell == 0])\n",
    "            if n_empty == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "    def play_vs_human(self):\n",
    "        print(\"Welcome to Tic-Tac-Toe. Your opponent is a minimax AI, who makes first turn randomly. \\n\"\n",
    "              \"Use numbers from 0 to 8 to make turns. Below is the map of numbers to cells.\")\n",
    "        print(\"\"\"\n",
    "         0 | 1 | 2\n",
    "        -  + - + -\n",
    "         3 | 4 | 5\n",
    "        -  + - + -\n",
    "         6 | 7 | 8\"\"\")\n",
    "        \"You play for x, your opponent plays for o\"\n",
    "        self.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            try:\n",
    "                self.render()\n",
    "                action = int(input(\">>> \"))\n",
    "            except:\n",
    "                print(\"could not parse your input. Please try again. Use numbers from 0 to 8 to make turns.\")\n",
    "            else:\n",
    "                row = action // 3\n",
    "                column = action % 3\n",
    "\n",
    "                valid_turn = self.try_make_turn(row, column)\n",
    "                if not valid_turn:\n",
    "                    print(\"Unfortunately you can't overwrite existing shapes. \\n\"\n",
    "                          \"You have made an invalid turn and therefore lost the game.\")\n",
    "                    break\n",
    "\n",
    "                result = self.evaluate(self.board)\n",
    "\n",
    "                if result is not None:\n",
    "                    self.render()\n",
    "                    if result == 1:\n",
    "                        print(\"Congratulations! you have won the game!\")\n",
    "                    elif result == 0:\n",
    "                        print(\"The game has ended in a draw.\")\n",
    "                    break\n",
    "\n",
    "                row, column = self.o_ai.decide_turn()\n",
    "                assert self.try_make_turn(row, column)\n",
    "                result = self.evaluate(self.board)\n",
    "\n",
    "                if result is not None:\n",
    "                    self.render()\n",
    "                    if result == -1:\n",
    "                        print(\"Minimax AI has won the game.\")\n",
    "                    elif result == 0:\n",
    "                        print(\"The game has ended in a draw.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        print(\"Thanks for playing Tic-Tac-Toe. Have a nice day and come back any time!\")\n",
    "\n",
    "\n",
    "    def render(self, mode=\"Human\"):\n",
    "        shapes = {-1: 'o', 0: ' ', 1: 'x'}\n",
    "        print(f\"{shapes[self.board[0,0]]} | {shapes[self.board[0,1]]} | {shapes[self.board[0,2]]}\")\n",
    "        print('- + - + -')\n",
    "        print(f\"{shapes[self.board[1,0]]} | {shapes[self.board[1,1]]} | {shapes[self.board[1,2]]}\")\n",
    "        print('- + - + -')\n",
    "        print(f\"{shapes[self.board[2,0]]} | {shapes[self.board[2,1]]} | {shapes[self.board[2,2]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY PLAYING VS MINIMAX\n",
    "ttt = TicTacToe()\n",
    "ttt.play_vs_human()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Create a benchmark\n",
    "We first create a benchmark by testing a random agent playing against Minimax AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST HOW GOOD RANDOM AI IS\n",
    "\n",
    "ttt = TicTacToe()\n",
    "\n",
    "x_ai = RandomAI(ttt)\n",
    "ttt.x_ai = x_ai\n",
    "\n",
    "o_ai = MinimaxAI(ttt)\n",
    "ttt.o_ai = o_ai\n",
    "\n",
    "\n",
    "counters = {-1:0, 0:0, 1:0}\n",
    "n_games = int(1e4)\n",
    "\n",
    "def run_trials():\n",
    "    for i in range(n_games):\n",
    "        if i % 50 == 0:\n",
    "            print(i, \"out of\", n_games)\n",
    "            print(f\"stats: out of {i} games, x has won {counters[1]} times, o - {counters[-1]} times, and there were \"\n",
    "                  f\"{counters[0]} draws.\")\n",
    "\n",
    "        ttt.reset()\n",
    "        result = ttt.play_ai_game()\n",
    "        if result < 0:\n",
    "            result = -1\n",
    "        elif result > 0:\n",
    "            result = 1\n",
    "        counters[result] += 1\n",
    "\n",
    "\n",
    "\n",
    "run_trials()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Train the reinforcement learning agent\n",
    "Next, we train a neural network to play against Minimax AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN DQN FOR TIC-TAC-TOE\n",
    "\n",
    "from __future__ import division\n",
    "import argparse\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "from gym.wrappers import TimeLimit\n",
    "env = TimeLimit(TicTacToe(), max_episode_steps=10)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build our model. We use the same model that was described by Mnih et al. (2015).\n",
    "WINDOW_LENGTH = 4\n",
    "input_shape = (3,3)\n",
    "input_shape = (WINDOW_LENGTH,) + input_shape\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(nb_actions))\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "# Select a policy. We use eps-greedy action selection, which means that a random action is selected\n",
    "# with probability eps. We anneal eps from 1.0 to 0.1 over the course of 1M steps. This is done so that\n",
    "# the agent initially explores the environment (high eps) and then gradually sticks to what it knows\n",
    "# (low eps). We also set a dedicated eps value that is used during testing. Note that we set it to 0.05\n",
    "# so that the agent still performs some random actions. This ensures that the agent cannot get stuck.\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)\n",
    "\n",
    "# The trade-off between exploration and exploitation is difficult and an on-going research topic.\n",
    "# If you want, you can experiment with the parameters or use a different policy. Another popular one\n",
    "# is Boltzmann-style exploration:\n",
    "# policy = BoltzmannQPolicy(tau=1.)\n",
    "# Feel free to give it a try!\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory, nb_steps_warmup=50000, gamma=.99,\n",
    "               target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "print(\"about to enter the flags branching.\")\n",
    "\n",
    "\n",
    "# Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
    "# can be prematurely aborted. Notice that you can the built-in Keras callbacks!\n",
    "weights_filename = 'dqn_ttt_weights.h5f'\n",
    "checkpoint_weights_filename = 'dqn_ttt_weights_{step}.h5f'\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=50000)]\n",
    "print(\"about to fit the dqn!\")\n",
    "# dqn.load_weights(weights_filename)\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=int(1e5), log_interval=10000, verbose=1)\n",
    "\n",
    "# After training is done, we save the final weights one more time.\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 10 episodes.\n",
    "dqn.test(env, nb_episodes=1, visualize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Reinforcement Learning agent class to use the same API as other AIs\n",
    "NeuralAI class - an agent API using the neural network we have trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE AGENT BASED ON THE DQN NET\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents.dqn import DQNAgent\n",
    "\n",
    "\n",
    "WINDOW_LENGTH = 4\n",
    "input_shape = (3, 3)\n",
    "input_shape = (WINDOW_LENGTH,) + input_shape\n",
    "nb_actions = 9\n",
    "\n",
    "\n",
    "class NeuralAI:\n",
    "\n",
    "    def __init__(self, game, weights_path=None):\n",
    "        self.game = game\n",
    "        model = self.build_model()\n",
    "        dqn = DQNAgent(model=model, nb_actions=nb_actions, gamma=.99,\n",
    "                       memory=SequentialMemory(limit=10, window_length=WINDOW_LENGTH))\n",
    "\n",
    "        dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "        import sys\n",
    "        sys.path.append(\"../\")\n",
    "        weights_filename = weights_path or 'dqn_ttt_weights.h5f'\n",
    "        dqn.load_weights(weights_filename)\n",
    "        self.dqn = dqn\n",
    "\n",
    "    def decide_turn(self):\n",
    "        board = self.game.board\n",
    "        action = self.dqn.forward(board)\n",
    "        row = action // 3\n",
    "        column = action % 3\n",
    "\n",
    "        return row, column\n",
    "\n",
    "    @staticmethod\n",
    "    def build_model():\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=input_shape))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(nb_actions))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Benchmark the trained agent\n",
    "\n",
    "The network is trained now. We benchmark it to compare it with random AI and Minimax AI; we also analyse how much computation do Minimax and the neural network agent require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST THE NEURAL AI\n",
    "\n",
    "ttt = TicTacToe()\n",
    "\n",
    "x_ai = NeuralAI(ttt)\n",
    "ttt.x_ai = x_ai\n",
    "\n",
    "o_ai = MinimaxAI(ttt)\n",
    "ttt.o_ai = o_ai\n",
    "\n",
    "\n",
    "counters = {-1:0, 0:0, 1:0}\n",
    "n_games = int(1e4)\n",
    "\n",
    "def run_trials():\n",
    "    for i in range(n_games):\n",
    "        if i % 50 == 0:\n",
    "            print(i, \"out of\", n_games)\n",
    "            print(f\"stats: out of {i} games, x has won {counters[1]} times, o - {counters[-1]} times, and there were \"\n",
    "                  f\"{counters[0]} draws.\")\n",
    "\n",
    "        ttt.reset()\n",
    "        result = ttt.play_ai_game()\n",
    "        if result < 0:\n",
    "            result = -1\n",
    "        elif result > 0:\n",
    "            result = 1\n",
    "        counters[result] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from cProfile import Profile\n",
    "profiler = Profile()\n",
    "profiler.runcall(run_trials)\n",
    "profiler.print_stats('cumulative')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
